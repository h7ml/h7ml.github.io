import{_ as p}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,o as c,c as o,d as n,e as s,a as t,f as a}from"./app-Cbix2SPG.js";const l={},r=a(`<h2 id="scrapy" tabindex="-1"><a class="header-anchor" href="#scrapy"><span>Scrapy</span></a></h2><h2 id="安装" tabindex="-1"><a class="header-anchor" href="#安装"><span>安装</span></a></h2><h3 id="windows-安装方式" tabindex="-1"><a class="header-anchor" href="#windows-安装方式"><span>Windows 安装方式</span></a></h3><p>升级 pip 版本：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pip
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>通过 pip 安装 Scrapy 框架:</p><h3 id="ubuntu-安装方式" tabindex="-1"><a class="header-anchor" href="#ubuntu-安装方式"><span>Ubuntu 安装方式</span></a></h3><p>安装非 Python 的依赖:</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>通过 pip 安装 Scrapy 框架：</p><h3 id="mac-os-安装方式" tabindex="-1"><a class="header-anchor" href="#mac-os-安装方式"><span>Mac OS 安装方式</span></a></h3><p>pip 版本必须转 22+，升级 pip 版本</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code> pip3 install --upgrade pip
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>使用清华源下载</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code> pip3 <span class="token function">install</span> <span class="token parameter variable">-i</span> https://pypi.tuna.tsinghua.edu.cn/simple scrapy
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><blockquote><p>注意：在 mac 中使用 scrapy 指令必须在前面加上 python3 -m</p></blockquote><h2 id="新建项目" tabindex="-1"><a class="header-anchor" href="#新建项目"><span>新建项目</span></a></h2><p>在开始爬取之前，必须创建一个新的 Scrapy 项目。进入自定义的项目目录中，运行下列命令：</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>scrapy startproject mySpider
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>其中， mySpider 为项目名称，可以看到将会创建一个 mySpider 文件夹，目录结构大致如下：</p><p>下面来简单介绍一下各个主要文件的作用：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>mySpider<span class="token operator">/</span>
    scrapy<span class="token punctuation">.</span>cfg
    mySpider<span class="token operator">/</span>
        __init__<span class="token punctuation">.</span>py
        items<span class="token punctuation">.</span>py
        pipelines<span class="token punctuation">.</span>py
        settings<span class="token punctuation">.</span>py
        spiders<span class="token operator">/</span>
            __init__<span class="token punctuation">.</span>py
            <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这些文件分别是:</p><ul><li>scrapy.cfg: 项目的配置文件。</li><li>mySpider/: 项目的 Python 模块，将会从这里引用代码。</li><li>mySpider/items.py: 项目的目标文件。</li><li>mySpider/pipelines.py: 项目的管道文件。</li><li>mySpider/settings.py: 项目的设置文件。</li><li>mySpider/spiders/: 存储爬虫代码目录。</li></ul><h3 id="创建爬虫文件" tabindex="-1"><a class="header-anchor" href="#创建爬虫文件"><span>创建爬虫文件</span></a></h3><p>1、cd 进入 <code>spiders</code> 文件夹中</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> mySpider<span class="token punctuation">\\</span>spiders<span class="token punctuation">\\</span>spiders
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>2、创建爬虫文件</p><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code><span class="token comment"># scrapy genspider 文件名  网页地址</span>
scrapy genspider <span class="token builtin class-name">test</span> www.baidu.com
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>3、<code>test.py</code></p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">TestSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">&#39;test&#39;</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;www.baidu.com&#39;</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;https://www.baidu.com/&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="运行爬虫代码" tabindex="-1"><a class="header-anchor" href="#运行爬虫代码"><span>运行爬虫代码</span></a></h3><blockquote><p>注意：有的网站会有 robots 协议，这是一个君子协议，scrapy 默认是启动遵守的，如果想要爬取需要关闭</p></blockquote><p>在<code>settings.py</code>文件中</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code># Obey robots.txt rules
ROBOTSTXT_OBEY = True 注释该行
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="语法" tabindex="-1"><a class="header-anchor" href="#语法"><span>语法</span></a></h2><h3 id="response-对象" tabindex="-1"><a class="header-anchor" href="#response-对象"><span>response 对象</span></a></h3><p><strong>解析数据返回的对象</strong></p><ul><li><p><code>response.body</code> ：响应返回页面已二进制格式的内容</p></li><li><p><code>response.text</code> ：响应返回页面已字符串格式的内容</p></li><li><p><code>response.url</code> ：响应返回页面 url</p></li><li><p><code>response.status</code> ：响应返回 ajax 请求状态码</p></li><li><p><code>response.xpath()</code>：（常用） 使用 xpath 路径查询特定元素，返回一个<code>selector</code>列表对象</p></li><li><p><code>response.css()</code>：使用<code>css_selector</code>查询元素，返回一个<code>selector</code>列表对象</p><ul><li>获取内容 ：<code>response.css(&#39;#su::text&#39;).extract_first()</code></li><li>获取属性 ：<code>response.css(&#39;#su::attr(“value”)&#39;).extract_first()</code></li></ul></li></ul><h3 id="selector-对象" tabindex="-1"><a class="header-anchor" href="#selector-对象"><span>selector 对象</span></a></h3><blockquote><p>通过<code>xpath</code>方法调用返回的是<code>seletor</code>列表</p></blockquote><h4 id="extract" tabindex="-1"><a class="header-anchor" href="#extract"><span>extract()</span></a></h4><ul><li>提取<code>selector</code>对象的值</li><li>如果提取不到值，那么会报错</li><li>使用 xpath 请求到的对象是一个<code>selector</code>对象，需要进一步使用<code>extract()</code>方法拆 包，转换为<code>unicode</code>字符串</li></ul><h4 id="extract-first" tabindex="-1"><a class="header-anchor" href="#extract-first"><span>extract_first()</span></a></h4><ul><li>提取<code>seletor</code>列表中的第一个值</li><li>如果提取不到值，会返回一个空值</li><li>返回第一个解析到的值，如果列表为空，此种方法也不会报错，会返回一个空值 <code>xpath() css()</code></li></ul><blockquote><p>注意：每一个<code>selector</code>对象可以再次的去使用<code>xpath</code>或者<code>css</code>方法</p></blockquote><h2 id="使用管道封装" tabindex="-1"><a class="header-anchor" href="#使用管道封装"><span>使用管道封装</span></a></h2>`,47),d={href:"http://items.py",target:"_blank",rel:"noopener noreferrer"},u=a(`<div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Scrapy01TestItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># define the fields for your item here like:</span>
    <span class="token comment"># name = scrapy.Field()</span>
    <span class="token comment"># src = scrapy.Field()</span>
    name <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    href <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2、爬虫住文件</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> scrapy_01_test<span class="token punctuation">.</span>items <span class="token keyword">import</span> Scrapy01TestItem


<span class="token keyword">class</span> <span class="token class-name">TestSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">&#39;test&#39;</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;bbs.mihoyo.com&#39;</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;https://bbs.mihoyo.com/ys/obc/channel/map/189/25?bbs_presentation_style=no_header&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        ul <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">&#39;//*[@id=&quot;__layout&quot;]/div/div[2]/div[2]/div/div[1]/div[2]/ul/li/div/ul/li[1]/div/div/a&#39;</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> li <span class="token keyword">in</span> ul<span class="token punctuation">:</span>
            href <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">&#39;@href&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
            name <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">&#39;.//div[2]/text()&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token comment"># 调用Scrapy01TestItem将数据存储中目标文件中</span>
            <span class="token comment"># data = Scrapy01TestItem(href=href, name=name)</span>

            <span class="token comment"># 第二页的地址</span>
            url <span class="token operator">=</span> <span class="token string">&#39;https://bbs.mihoyo.com&#39;</span> <span class="token operator">+</span> href

            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_second<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token punctuation">{</span>
                <span class="token string">&#39;href&#39;</span><span class="token punctuation">:</span> href<span class="token punctuation">,</span>
                <span class="token string">&#39;name&#39;</span><span class="token punctuation">:</span> name
            <span class="token punctuation">}</span><span class="token punctuation">)</span>

            <span class="token comment"># 每次循环得到的结果交给管道，如果是多个管道链接调用泽在最后一个执行的管道中 yield 最终的数据</span>
            <span class="token comment"># yield data</span>

    <span class="token keyword">def</span> <span class="token function">parse_second</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        src <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span>
            <span class="token string">&#39;//*[@id=&quot;__layout&quot;]/div/div[2]/div[2]/div/div[1]/div[3]/div[3]/div[3]/div[1]/div[1]/div/ul[2]/li/img/@src&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        name <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">&#39;name&#39;</span><span class="token punctuation">]</span>
        href <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">&#39;href&#39;</span><span class="token punctuation">]</span>

        <span class="token comment"># 调用Scrapy01TestItem将数据存储中目标文件中</span>
        data <span class="token operator">=</span> Scrapy01TestItem<span class="token punctuation">(</span>src<span class="token operator">=</span>src<span class="token punctuation">,</span> href<span class="token operator">=</span>href<span class="token punctuation">,</span> name<span class="token operator">=</span>name<span class="token punctuation">)</span>
        <span class="token comment"># 每次循环得到的结果交给管道，如果是多个管道链接调用泽在最后一个执行的管道中 yield 最终的数据</span>
        <span class="token keyword">yield</span> data
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3、在<code>settings.py</code>中开启管道</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># Configure item pipelines</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{</span>
   <span class="token comment"># 管道可以有多个</span>
   <span class="token comment"># scrapy_01_test.pipelines.Scrapy01TestPipeline 管道的类名路径</span>
   <span class="token comment"># 300 是管道的优先级，范围1-1000，值越小优先级越高</span>
   <span class="token string">&#39;scrapy_01_test.pipelines.Scrapy01TestPipeline&#39;</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,5),k={href:"http://pipelines.py",target:"_blank",rel:"noopener noreferrer"},m=a(`<div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># Define your item pipelines here</span>
<span class="token comment">#</span>
<span class="token comment"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="token comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="token comment"># useful for handling different item types with a single interface</span>
<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request

<span class="token keyword">from</span> itemadapter <span class="token keyword">import</span> ItemAdapter


<span class="token comment"># 必须在setings中开启管道才能使用</span>
<span class="token keyword">class</span> <span class="token class-name">Scrapy01TestPipeline</span><span class="token punctuation">:</span>
    <span class="token comment"># 在爬虫文件执行之前调用一次</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fb <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>initdata <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>

    <span class="token comment"># itme 就是在yield后面的对象</span>
    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>initdata<span class="token punctuation">.</span>append<span class="token punctuation">(</span>item<span class="token punctuation">)</span>
        <span class="token keyword">return</span> item

    <span class="token comment"># 在爬虫文件执行之后调用一次</span>
    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># w 模式每次执行都会打开文件覆盖之前的内容</span>
        self<span class="token punctuation">.</span>fb <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&#39;data.json&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;a&#39;</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">&#39;utf-8&#39;</span><span class="token punctuation">)</span>
        <span class="token comment"># write 方法必须写一个字符串</span>
        self<span class="token punctuation">.</span>fb<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>initdata<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 关闭</span>
        self<span class="token punctuation">.</span>fb<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 多管道开始</span>
<span class="token comment">#    在 settings 中开启管道</span>
<span class="token comment">#    &#39;scrapy_01_test.pipelines.DownLoadYS&#39;: 301,</span>
<span class="token keyword">class</span> <span class="token class-name">DownLoadYS</span><span class="token punctuation">:</span>
    <span class="token comment"># itme 就是在yield后面的对象</span>
    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        url <span class="token operator">=</span> item<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">&#39;src&#39;</span><span class="token punctuation">)</span>
        filename <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f&#39;./img/</span><span class="token interpolation"><span class="token punctuation">{</span>item<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">&quot;name&quot;</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">.jpg&#39;</span></span>

        urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> filename<span class="token operator">=</span>filename<span class="token punctuation">)</span>

        <span class="token keyword">return</span> item
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>5、items 目标文件</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># Define here the models for your scraped items</span>
<span class="token comment">#</span>
<span class="token comment"># See documentation in:</span>
<span class="token comment"># https://docs.scrapy.org/en/latest/topics/items.html</span>

<span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">Scrapy01TestItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># define the fields for your item here like:</span>
    <span class="token comment"># name = scrapy.Field()</span>
    src <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    name <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    href <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,3),v={id:"scrapy-crawlspideropen-in-new-window",tabindex:"-1"},b={class:"header-anchor",href:"#scrapy-crawlspideropen-in-new-window"},h={href:"https://geek-docs.com/scrapy/scrapy-tutorials/scrapy-crawlspider.html",target:"_blank",rel:"noopener noreferrer"},y=a(`<h3 id="创建爬虫文件-1" tabindex="-1"><a class="header-anchor" href="#创建爬虫文件-1"><span>创建爬虫文件</span></a></h3><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code># scrapy genspider 文件名  网页地址
scrapy genspider -t crawl test www.baidu.com
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Scrapy CrawlSpider</strong>，继承自<code>Spider</code>, 爬取网站常用的爬虫，其定义了一些规则(rule)方便追踪或者是过滤<code>link</code>。 也许该 spider 并不完全适合您的特定网站或项目，但其对很多情况都是适用的。 因此您可以以此为基础，修改其中的方法，当然您也可以实现自己的<code>spider</code>。</p><h3 id="class-scrapy-contrib-spiders-crawlspider" tabindex="-1"><a class="header-anchor" href="#class-scrapy-contrib-spiders-crawlspider"><span>class scrapy.contrib.spiders.CrawlSpider</span></a></h3><p><code>CrawlSpider</code>继承自<code>Spider</code>, 爬取网站常用的爬虫，其定义了一些规则(<code>rule</code>)方便追踪或者是过滤 link。 也许该 spider 并不完全适合您的特定网站或项目，但其对很多情况都是适用的。 因此您可以以此为基础，修改其中的方法，当然您也可以实现自己的<code>spider</code>。</p><p>除了从<code>Spider</code>继承过来的(您必须提供的)属性外，其提供了一个新的属性:</p><ul><li><strong>rules</strong></li></ul><p>一个包含一个(或多个) <code>Rule</code> 对象的集合(list)。 每个 <code>Rule</code> 对爬取网站的动作定义了特定表现。 <code>Rule</code>对象在下边会介绍。 如果多个 rule 匹配了相同的链接，则根据他们在本属性中被定义的顺序，第一个会被使用。</p><p>该 spider 也提供了一个可复写(<code>overrideable</code>)的方法:</p><ul><li><strong>parse_start_url(response)</strong></li></ul><p>当<code>start_url</code>的请求返回时，该方法被调用。 该方法分析最初的返回值并必须返回一个 <code>Item</code> 对象或者 一个 <code>Request</code> 对象或者 一个可迭代的包含二者对象。</p><h3 id="爬取规则" tabindex="-1"><a class="header-anchor" href="#爬取规则"><span>爬取规则</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">scrapy</span><span class="token punctuation">.</span>contrib<span class="token punctuation">.</span>spiders<span class="token punctuation">.</span>Rule<span class="token punctuation">(</span>link_extractor<span class="token punctuation">,</span> callback<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> cb_kwargs<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> follow<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> process_links<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> process_request<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p><code>link_extractor</code> 是一个 <code>Link Extractor</code> 对象。 其定义了如何从爬取到的页面提取链接。</p><p><code>callback</code> 是一个 callable 或 string(该 spider 中同名的函数将会被调用)。 从<code>link_extractor</code>中每获取到链接时将会调用该函数。该回调函数接受一个 response 作为其第一个参数， 并返回一个包含 Item 以及(或) Request 对象(或者这两者的子类)的列表(list)。</p><blockquote><p>当编写爬虫规则时，请避免使用 <code>parse</code> 作为回调函数。 由于 <code>CrawlSpider</code> 使用 parse 方法来实现其逻辑，如果 您覆盖了 parse 方法，crawl spider 将会运行失败。</p></blockquote><ul><li><code>cb_kwargs</code>: 包含传递给回调函数的参数(keyword argument)的字典。</li><li><code>follow</code>: 是一个布尔(boolean)值，指定了根据该规则从 response 提取的链接是否需要跟进。 如果 callback 为 None， follow 默认设置为 True ，否则默认为 False 。</li><li><code>process_links</code>: 是一个 callable 或 string(该 spider 中同名的函数将会被调用)。 从 link_extractor 中获取到链接列表时将会调用该函数。该方法主要用来过滤。</li><li><code>process_request</code>: 是一个 callable 或 string(该 spider 中同名的函数将会被调用)。 该规则提取到每个 request 时都会调用该函数。该函数必须返回一个 request 或者 None。 (用来过滤 request)</li></ul><h3 id="crawlspider-示例" tabindex="-1"><a class="header-anchor" href="#crawlspider-示例"><span>CrawlSpider 示例</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>spiders <span class="token keyword">import</span> CrawlSpider<span class="token punctuation">,</span> Rule
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>linkextractors <span class="token keyword">import</span> LinkExtractor

<span class="token keyword">class</span> <span class="token class-name">MySpider</span><span class="token punctuation">(</span>CrawlSpider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">&#39;geek-docs&#39;</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;yiibai.com&#39;</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;https://www.yiibai.com/cplusplus/what-is-cpp.html&#39;</span><span class="token punctuation">]</span>

    rules <span class="token operator">=</span> <span class="token punctuation">(</span>
        <span class="token comment"># 提取匹配 &#39;yiibai.com/cplusplus&#39; (但不匹配 &#39;subsection.php&#39;) 的链接并跟进链接(没有callback意味着follow默认为True)</span>
        Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">&#39;yiibai.com/cplusplus&#39;</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span> deny<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">&#39;subsection\\.php&#39;</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>

        <span class="token comment"># 提取匹配 &#39;item.php&#39; 的链接并使用spider的parse_item方法进行分析</span>
        Rule<span class="token punctuation">(</span>LinkExtractor<span class="token punctuation">(</span>allow<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">&#39;item\\.php&#39;</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> callback<span class="token operator">=</span><span class="token string">&#39;parse_item&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">&#39;Hi, this is an item page! %s&#39;</span> <span class="token operator">%</span> response<span class="token punctuation">.</span>url<span class="token punctuation">)</span>

        item <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        item<span class="token punctuation">[</span><span class="token string">&#39;id&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">&#39;//td[@id=&quot;item_id&quot;]/text()&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>re<span class="token punctuation">(</span><span class="token string">r&#39;ID: (\\d+)&#39;</span><span class="token punctuation">)</span>
        item<span class="token punctuation">[</span><span class="token string">&#39;name&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">&#39;//td[@id=&quot;item_name&quot;]/text()&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        item<span class="token punctuation">[</span><span class="token string">&#39;description&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">&#39;//td[@id=&quot;item_description&quot;]/text()&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>CrawlSpider</code>将从<code>yiibai.com</code>的首页开始爬取，获取<code>yiibai.com/cplusplus</code>以及<code>item</code>的链接并，对后者调用 <strong>parse_item</strong> 方法。 当 item 获得返回<code>response</code>时，将使用<code>XPath</code>处理 HTML 并生成一些数据填入 Item 中。</p><h2 id="日志信息和日志等级" tabindex="-1"><a class="header-anchor" href="#日志信息和日志等级"><span>日志信息和日志等级</span></a></h2><p><strong>日志级别：</strong></p><ol><li>CRIRICAL：严重错误</li><li>ERROR：一般错误</li><li>WARNING：警告</li><li>INFO：一级信息</li><li>DEBUG：调试信息</li></ol><p>默认日志等级是 DEBUG，只有出现 DEBUG 才会出现。</p>`,24),g={href:"http://settings.py",target:"_blank",rel:"noopener noreferrer"},f=a(`<ul><li>LOG_FILE：将屏幕显示的信息全部记录到文件中，屏幕不在显示，注意文件名一定是.log</li><li>LOG_LEVEL: 设置日志的等级，就显示哪些，不显示哪些</li></ul><h2 id="post-请求" tabindex="-1"><a class="header-anchor" href="#post-请求"><span>POST 请求</span></a></h2><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> scrapy

<span class="token keyword">class</span> <span class="token class-name">FySpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">&#39;fy&#39;</span>
    <span class="token comment"># allowed_domains = [&#39;www.baidu.com&#39;]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;https://fanyi.baidu.com/sug&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        data<span class="token operator">=</span><span class="token punctuation">{</span>
            <span class="token string">&#39;kw&#39;</span><span class="token punctuation">:</span><span class="token string">&quot;beautiful&quot;</span>
        <span class="token punctuation">}</span>
        <span class="token keyword">for</span> url <span class="token keyword">in</span> self<span class="token punctuation">.</span>start_urls<span class="token punctuation">:</span>
            <span class="token keyword">yield</span>  scrapy<span class="token punctuation">.</span>FormRequest<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span>formdata<span class="token operator">=</span>data<span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,3);function w(_,x){const e=i("ExternalLinkIcon");return c(),o("div",null,[r,n("p",null,[s("1、"),n("a",d,[s("items.py"),t(e)]),s(" 在项目目标文件中定义")]),u,n("p",null,[s("4、"),n("a",k,[s("pipelines.py"),t(e)]),s(" 管道文件")]),m,n("h2",v,[n("a",b,[n("span",null,[n("a",h,[s("Scrapy CrawlSpideropen in new window"),t(e)])])])]),y,n("p",null,[n("strong",null,[n("a",g,[s("settings.py"),t(e)]),s(" 文件设置")]),s("：")]),f])}const T=p(l,[["render",w],["__file","scrapy.html.vue"]]),I=JSON.parse('{"path":"/posts/Python/scrapy.html","title":"Scrapy","lang":"zh-CN","frontmatter":{"icon":"python","order":2,"date":"2022-05-20T00:00:00.000Z","author":"h7ml","category":"python","tag":"python","title":"Scrapy","description":"Scrapy 安装 Windows 安装方式 升级 pip 版本： 通过 pip 安装 Scrapy 框架: Ubuntu 安装方式 安装非 Python 的依赖: 通过 pip 安装 Scrapy 框架： Mac OS 安装方式 pip 版本必须转 22+，升级 pip 版本 使用清华源下载 注意：在 mac 中使用 scrapy 指令必须在前面加上...","head":[["link",{"rel":"canonical","href":"https://www.h7ml.cn/posts/Python/scrapy.html"}],["meta",{"property":"og:url","content":"https://www.h7ml.cn/posts/Python/scrapy.html"}],["meta",{"property":"og:site_name","content":"h7ml-前端物语"}],["meta",{"property":"og:title","content":"Scrapy"}],["meta",{"property":"og:description","content":"Scrapy 安装 Windows 安装方式 升级 pip 版本： 通过 pip 安装 Scrapy 框架: Ubuntu 安装方式 安装非 Python 的依赖: 通过 pip 安装 Scrapy 框架： Mac OS 安装方式 pip 版本必须转 22+，升级 pip 版本 使用清华源下载 注意：在 mac 中使用 scrapy 指令必须在前面加上..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-05-03T04:52:44.000Z"}],["meta",{"property":"article:author","content":"h7ml"}],["meta",{"property":"article:tag","content":"python"}],["meta",{"property":"article:published_time","content":"2022-05-20T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-05-03T04:52:44.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Scrapy\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-05-20T00:00:00.000Z\\",\\"dateModified\\":\\"2023-05-03T04:52:44.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"h7ml\\"}]}"]]},"headers":[{"level":2,"title":"Scrapy","slug":"scrapy","link":"#scrapy","children":[]},{"level":2,"title":"安装","slug":"安装","link":"#安装","children":[{"level":3,"title":"Windows 安装方式","slug":"windows-安装方式","link":"#windows-安装方式","children":[]},{"level":3,"title":"Ubuntu 安装方式","slug":"ubuntu-安装方式","link":"#ubuntu-安装方式","children":[]},{"level":3,"title":"Mac OS 安装方式","slug":"mac-os-安装方式","link":"#mac-os-安装方式","children":[]}]},{"level":2,"title":"新建项目","slug":"新建项目","link":"#新建项目","children":[{"level":3,"title":"创建爬虫文件","slug":"创建爬虫文件","link":"#创建爬虫文件","children":[]},{"level":3,"title":"运行爬虫代码","slug":"运行爬虫代码","link":"#运行爬虫代码","children":[]}]},{"level":2,"title":"语法","slug":"语法","link":"#语法","children":[{"level":3,"title":"response 对象","slug":"response-对象","link":"#response-对象","children":[]},{"level":3,"title":"selector 对象","slug":"selector-对象","link":"#selector-对象","children":[]}]},{"level":2,"title":"使用管道封装","slug":"使用管道封装","link":"#使用管道封装","children":[]},{"level":2,"title":"Scrapy CrawlSpideropen in new window","slug":"scrapy-crawlspideropen-in-new-window","link":"#scrapy-crawlspideropen-in-new-window","children":[{"level":3,"title":"创建爬虫文件","slug":"创建爬虫文件-1","link":"#创建爬虫文件-1","children":[]},{"level":3,"title":"class scrapy.contrib.spiders.CrawlSpider","slug":"class-scrapy-contrib-spiders-crawlspider","link":"#class-scrapy-contrib-spiders-crawlspider","children":[]},{"level":3,"title":"爬取规则","slug":"爬取规则","link":"#爬取规则","children":[]},{"level":3,"title":"CrawlSpider 示例","slug":"crawlspider-示例","link":"#crawlspider-示例","children":[]}]},{"level":2,"title":"日志信息和日志等级","slug":"日志信息和日志等级","link":"#日志信息和日志等级","children":[]},{"level":2,"title":"POST 请求","slug":"post-请求","link":"#post-请求","children":[]}],"git":{"createdTime":1683089564000,"updatedTime":1683089564000,"contributors":[{"name":"h7ml","email":"h7ml@qq.com","commits":1}]},"readingTime":{"minutes":7.91,"words":2372},"filePathRelative":"posts/Python/scrapy.md","localizedDate":"2022年5月20日","excerpt":"<h2>Scrapy</h2>\\n<h2>安装</h2>\\n<h3>Windows 安装方式</h3>\\n<p>升级 pip 版本：</p>\\n<div class=\\"language-bash\\" data-ext=\\"sh\\" data-title=\\"sh\\"><pre class=\\"language-bash\\"><code>pip <span class=\\"token function\\">install</span> <span class=\\"token parameter variable\\">--upgrade</span> pip\\n</code></pre></div><p>通过 pip 安装 Scrapy 框架:</p>","autoDesc":true}');export{T as comp,I as data};
